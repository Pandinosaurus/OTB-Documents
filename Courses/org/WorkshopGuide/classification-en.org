** Supervised classification of a satellite image time series        :slides:
*** Objectives and Data
**** Objectives
     Objectives are as follows:
     - Being able to perform a supervised classification
     - Being able to measure the classification performance
     - Knowing existing post-processing for classification
**** Data
     Data are available in the ~Data/classification~ folder, with the
     following sub-folders:
     - ~images~ contains the Sentinel2 time series,
     - ~references~ contains the reference data for training and
       validation (ESRI Shapefile format),
     - ~support~ contains different useful files for the workshop
       (for instance QGIS style files)
     
*** Steps
    Workshop follows the following steps:
    1. Introduction to the data
    2. Single date training
    3. Spot the date with the best performance
    4. Classifying and producing a colored classification map
    5. Evaluate global performance
    6. Classification regularization
    7. Classification with multiple dates
       
*** Introduction to Sentinel-2 data

    *Spatial resolution:* 20 meters (source: 10 meters)
    *Tile:* T31TCJ (extract)
|------------|
| 2016-06-07 |
| 2016-07-07 |
| 2016-08-06 |
| 2016-09-05 |
| 2016-10-05 |
|------------|

*** Introduction to  Sentinel-2 data

    *Spatial resolution:* 20 meters (source: 10 meters)
    *Tile:* T31TCJ (extract)

|---+----------------+------------+------------+--------------------|
| # | Band name      | S2 band id | Wavelength | Initial resolution |
|---+----------------+------------+------------+--------------------|
| 0 | Blue           | B2         | 490 nm     | 10 m               |
| 1 | Green          | B3         | 560 nm     | 10 m               |
| 2 | Red            | B4         | 665 nm     | 10 m               |
| 3 | NIR - Narrow 1 | B5         | 705 nm     | 20 m               |
| 4 | NIR - Narrow 2 | B6         | 740 nm     | 20 m               |
| 5 | NIR - Narrow 3 | B7         | 783 nm     | 20 m               |
| 6 | NIR - Wide     | B8         | 842 nm     | 10 m               |
| 7 | NIR - Narrow 4 | B8A        | 865 nm     | 20 m               |
| 8 | SWIR 1         | B11        | 1610 nm    | 20 m               |
| 9 | SWIR 2         | B12        | 2190 nm    | 20 m               |
|---+----------------+------------+------------+--------------------|

*** Introduction to reference data

|------+-----------------------------+---------------------+--------------------|
| Code | Name                        | #polygons training  | #polygons testing  |
|------+-----------------------------+---------------------+--------------------|
|   10 | Annual crops                | 3129                | 3078               |
|   31 | Deciduous Forests           | 176                 | 292                |
|   32 | Evergreen Forests           | 23                  | 29                 |
|   34 | Grass                       | 2                   | 2                  |
|   36 | Woody Moorlands             | 63                  | 38                 |
|   41 | Dense Urban Area            | 30                  | 33                 |
|   42 | Light Urban Area            | 326                 | 239                |
|   43 | Industrial Area             | 154                 | 212                |
|   44 | Roads                       | 162                 | 114                |
|   51 | Water                       | 243                 | 332                |
|  211 | Meadow                      | 320                 | 311                |
|  221 | Orchards                    | 227                 | 254                |
|  222 | Vineyards                   | 129                 | 97                 |
|------+-----------------------------+---------------------+--------------------|



*** Supervised classification
    #+ATTR_LATEX: :float t :width \textwidth
    [[file:Images/classification.png]]
   
*** RF algorithm
    A committee of random decision trees.
 
**** Learning
     1. Split the learning set in k random sets $S_k$
     2. For each $S_k$, randomly choose $F_k$ features
     3. Recursively build a decision tree, for each node:
        1. Choose $f \in F_k$ and the threshold $t_k$ which partitions the remaining set in two subsets as pure as possible
        2. Stop when the remaining set is too small
 
**** Decision
     Majority vote between all random trees

*** Confusion matrix

|--------+-------------+-------------+-------------|
|        | Pred. 1     | Pred. 2     | Pred. 3     |
|--------+-------------+-------------+-------------|
| Ref. 1 | True pos. 1 |             |             |
| Ref. 2 |             | True pos. 2 |             |
| Ref. 3 |             |             | True pos. 3 |
|--------+-------------+-------------+-------------|

- $precision = \frac{VP i}{\sum pred. i}$
- $recall = \frac{VP i}{T\sum ref. i}$
- $Accuracy = \frac{\sum{VP i}}{Total}$
- $Kappa = \frac{Accuracy - chance}{1-chance}$
  



** Supervised classification of a satellite image time series         :guide:
*** Description                                                        :desc:
**** Summary

     This exercise allows to get familiar with supervised, pixel-wise
     classification applications in Orfeo ToolBox, using a Sentinel-2
     time series and a reference dataset for supervision.

**** Pre-requisites
     
     - Software installed (Monteverdi and Orfeo ToolBox)
     - Data downloaded
     - Basic knowledge on using applications in OTB
     - Basic knowledge on supervised classification
     
**** Objectives

     Objectives are the following:
     - Knowing the different applications of the supervised classification process
     - Using different learning algorithms
     - Measuring classification performances
     - Post-processing of classification

*** Steps                                                             :steps:

    Data are available in the ~Data/classification~ folder, with following sub-folders:
    - ~images~ contains the Sentinel-2 time series,
    - ~references/training~ contains training data in /shp/ format,
    - ~references/testing~ contains testing data in /shp/ format

**** Introduction to Sentinel-2 data
     
     In the data package, folder ~Data/classification/images~ contains
     5 Sentinel-2 images, extracted on tile T31TCJ, at the following dates:
    
|------------|
| 2016-06-07 |
| 2016-07-07 |
| 2016-08-06 |
| 2016-09-05 |
| 2016-10-05 |
|------------|


    Those images are multispectral, with 10 bands resampled at 20 m:

|---+----------------+------------+------------+--------------------|
| # | Band name      | S2 band id | Wavelength | Initial resolution |
|---+----------------+------------+------------+--------------------|
| 0 | Blue           | B2         | 490 nm     | 10 m               |
| 1 | Green          | B3         | 560 nm     | 10 m               |
| 2 | Red            | B4         | 665 nm     | 10 m               |
| 3 | NIR - Narrow 1 | B5         | 705 nm     | 20 m               |
| 4 | NIR - Narrow 2 | B6         | 740 nm     | 20 m               |
| 5 | NIR - Narrow 3 | B7         | 783 nm     | 20 m               |
| 6 | NIR - Wide     | B8         | 842 nm     | 10 m               |
| 7 | NIR - Narrow 4 | B8A        | 865 nm     | 20 m               |
| 8 | SWIR 1         | B11        | 1610 nm    | 20 m               |
| 9 | SWIR 2         | B12        | 2190 nm    | 20 m               |
|---+----------------+------------+------------+--------------------|


We therefore have 50 bands for each pixels. Images are encoded over 16 bits.

Open one image in monteverdi and set the bands for a true color
display (red, green, blue)

Open the remaining four images and look for changes.

*Note:* The QGIS style file ~support/images.qml~ can be loaded into
QGIS to set the rendering and color channels for each image.

Files ~references/training/training.shp~ 
and
~references/testing/testing.shp~ contain polygons defining 13 classes
over the scene:

|------+-----------------------------+---------------------+--------------------|
| Code | Name                        | #polygons training  | #polygons testing  |
|------+-----------------------------+---------------------+--------------------|
|   10 | Annual crops                | 3129                | 3078               |
|   31 | Deciduous Forests           | 176                 | 292                |
|   32 | Evergreen Forests           | 23                  | 29                 |
|   34 | Grass                       | 2                   | 2                  |
|   36 | Woody Moorlands             | 63                  | 38                 |
|   41 | Dense Urban Area            | 30                  | 33                 |
|   42 | Light Urban Area            | 326                 | 239                |
|   43 | Industrial Area             | 154                 | 212                |
|   44 | Roads                       | 162                 | 114                |
|   51 | Water                       | 243                 | 332                |
|  211 | Meadow                      | 320                 | 311                |
|  221 | Orchards                    | 227                 | 254                |
|  222 | Vineyards                   | 129                 | 97                 |
|------+-----------------------------+---------------------+--------------------|

Open one of the files in QGIS. The attribute table can be accessed by
    right-clicking on the layer -> /Open attributes table/. Each label
    is visible, and the list can be filtered with SQL expressions.

    *Note :* There is a style file ~support/polygons.qml~ that can be
    loaded into QGIS to colorize polygons according to their classes.
    
    Polygons are split into two sets: training and validation.

**** Single date training
     
     We are going to use the *TrainImagesClassifier* application in
     order to do supervised learning from the training date in
     ~references/training/training.shp~. First, we are going to work
     with the image from the 07.06.2016.

     The *TrainImagesClassifier* application will sample some image
     pixels within the training polygons, in order to build a
     well-balanced training set. This set is then passed to the
     learning algorithm.
     
     This application has some mandatory parameters:
     - The input images, which bands will be used as features by the
       learning algorithm,
     - The vector layer containing references polygons,
     - The name of the field in that layer that contains the class
       identifier,
     - The output file containing the learning model (call it ~model.rf~).

     Some optional parameters should also be set as follows:
     - Random forest classifier for the learning algorithm,
     - The maximal tree depth to 20,
     - The minimum number of samples in each node to 70,
     - The number of clusters to 13 (equal to the number of classes),
     - The maximum number of tree set to 50

     Look at the application logs, in particular the confusion matrix,
     the Kappa coefficients, and scores per class. What do you think
     of those results ? Without using polygons dedicated to
     validation, the application will use a part of the generated
     samples for validation. What can be deduced regarding the
     displayed performances ?

     Do the training again, but this time also use the validation
     dataset in ~reference/testing/testing.shp~ as validation layer
     (you therefore set two different shp files in the
     application). What can be observed ?
     
     Do the training again, and deactivate the ~temporary files cleaning~ option. Look
     at the intermediate data that have been generated. What are they
     used for ?

**** Spot the date with the best performance
     
     Do the training again, this time for each image date. What is
     the date with the best performances ? Does the Kappa coefficient
     change a lot ?
     
     Keep the ~model.rf~ file corresponding to the best date.

**** Classifying and producing a colored classification map

     Use the *ImageClassifier* application to produce the
     classification map corresponding to the best date (the one from
     05.09.2016). Be careful to use the model file training with this date.
     
     The output map is a TIFF image where each pixel value corresponds
     to the class. To visualize such images, the *ColorMapping*
     application allows to set a given color for each class.

     Use the *custom* mode from the *ColorMapping* application with
     the look-up table in ~support/color_map.txt~ to produced the
     colored map.
     
     *Note :* The image may not display correctly in QGIS, because of
     default no data value set in the file. No data can be deactivated
     in QGIS layer properties dialog.
     
**** Evaluate global performance

     We are now going to use the *ComputeConfusionMatrix* application
     in order to compute global classification performances. With
     respect to the performance evaluation during the training step,
     this application allows to:
     - Take into account all pixels in the reference date,
     - Evaluate performances of a post-processed classification map
       (for instance with regularization).
     The ~ref.vector.field CODE~ parameter is mandatory to indicate
    the field corresponding to class ids.

    Compute the global performance of the classification. What do you
    observe with respect to the performance computed during training ?
    How to explain this phenomena ?
     
**** Classification regularization
     
     We are now going to use the *ClassificationMapRegularization*
     application. It filters the classification map using a local
     majority voting scheme.

     Parameters are:$
     
     - ip.radius 1 :: Radius of the voting zone
     - ip.suvbool 0 :: What to do in case of tie vote. 0 is to keep
                       the initial value.

     Filter the result of previous classification. Evaluate the
     performances of the filtered map. What do you observe ?
      
**** Classification with multiple dates

     We are now going to use all dates for classifications. For this,
     you can use the ~images/all.vrt~ which contains all bands from
     each date concatenated (the image has therefore 50 bands).
     
     Do the whole workshop, but this time with this 50 bands
     image. What do temporal series bring to the performance of
     classification ?

     Compare both results in QGIS.

**** Going further
     
     1) Can we obtain better performances with other classification
        algorithms ?

     2) In QGIS, merge in reference data the grass and Woody
        Moorlands. Are the performances better ?

     3) The ~TrainImagesClassifier~ also has an unsupervised algorithm
        (Shark KMeans). Compare results with supervised and
        unsupervised classification.

** Supervised classification of a satellite image time series     :solutions:

In the following solution, the ~$DATA~ environment variable correspond
to the folder containing the workshop data.

*** Single date training

The single date training can be achieved with the following command line:

#+BEGIN_EXAMPLE
$ otbcli_TrainImagesClassifier -io.il $DATA/images/20160607_T31TCJ_ROI_20m.tif \
                               -io.vd $DATA/references/training/training.shp   \
                               -io.out model.rf                                \
                               -sample.vfn CODE -classifier rf                 \
                               -classifier.rf.nbtrees 50 -classifier.rf.max 20 \
                               -classifier.rf.cat 13
#+END_EXAMPLE

This first run outputs the following results:

#+LATEX: \begin{small}
#+BEGIN_EXAMPLE
Confusion matrix (rows = reference labels, columns = produced labels):
       [10]  [31]  [32]  [34]  [36]  [41]  [42 ] [43]  [44]  [51] [211] [221] [222] 
[ 10]   374     3     0    26     1     6    19    11     2     0    13    10    13 
[ 31]     0   436     5     7    14     0     0     0     0     0     7     8     1 
[ 32]     3    16   420     6    15     0     0     0     0     0    12     3     3 
[ 34]    30    16    21   268    41     1    13     2     1     0    27    18    40 
[ 36]    10     6    13    31   336     0     7     0     0     0    42    13    20 
[ 41]     5     0     0     0     0   388    49    21    13     0     0     1     1 
[ 42]    31     1     3    10     3    44   288    36    22     0     0     5    35 
[ 43]    18     0     2    11     1    37    59   227   114     1     0     7     1 
[ 44]     7     0     3     3     2     5    10    71   371     0     0     5     1 
[ 51]     0     0     6     0     0     0     0     0     1   470     0     1     0 
[211]    19     7    13    41    64     0     3     0     0     0   266    14    51 
[221]    18    18     8    13    23     1    11     4     3     0    38   332     9 
[222]    22     0     1    30    12     0    14     0     0     0    16     4   379

[...]

Global performance, Kappa index: 0.710774

#+END_EXAMPLE
#+Latex: \end{small}

The displayed performance is rather optimistic, since the samples used
to estimate it comes from the same polygons as for the training. To
obtain a more realistic evaluation of performances, it is better to
select a dedicated polygon set for validation.

#+BEGIN_EXAMPLE
$ otbcli_TrainImagesClassifier -io.il $DATA/images/20160607_T31TCJ_ROI_20m.tif \
                               -io.vd $DATA/references/training/training.shp   \
                               -io.valid $DATA/references/testing/testing.shp  \
                               -io.out model.rf                                \
                               -sample.vfn CODE -classifier rf                 \
                               -classifier.rf.nbtrees 50 -classifier.rf.max 20 \
                               -classifier.rf.cat 13
#+END_EXAMPLE

Which outputs the following results:

#+LATEX: \begin{small}
#+BEGIN_EXAMPLE
Confusion matrix (rows = reference labels, columns = produced labels):
       [10]  [31]  [32]  [34]  [36]  [41]  [42]  [43]  [44]  [51] [211] [221] [222] 
[ 10]   795     6     6    47     9     4    13    18     0     0    22    10    23 
[ 31]     1   777    38    14    42     0     0     0     0     1    59    21     0 
[ 32]     1    34   865     3    12     2    14     1     1     0     2    15     3 
[ 34]    50   273   120    72    51     0     8     0     0    49   105   157    68 
[ 36]    23    27    45   186   336     0     1     1     0     0   215    79    40 
[ 41]     4     0     1     1     0   665   176    53    49     0     1     1     2 
[ 42]    20     1     3    11     2    98   652    75    43     0     5     8    35 
[ 43]    21     0     1    19     5    44   207   464   146     1     8    17    20 
[ 44]     7     0     1     3     0    13    23   240   662     0     0     4     0 
[ 51]     0     0     1     0     0     0     0     3     1   945     0     3     0 
[211]    81    21    17    81   112     1    16     4     0     0   507    40    73 
[221]    46    51    22    43    42     0    18    10     2     2   107   541    69 
[222]    70     0     0    68    23     0    71     1     0     0    45    11   664

[...]

Global performance, Kappa index: 0.611403
#+END_EXAMPLE
#+Latex: \end{small}

If the ~cleanup~ option is deactivated by adding ~-cleanup 0~
parameter, the application does not erase temporary outputs.

The following XML files contain the statistics of available samples
for each class, for training and validation set:
- ~model.rf_statsTrain_1.xml~
- ~model.rf_statsValid_1.xml~


The following shapefile files contain the samples used for training and for validation:
- ~model.rf_samplesTrain_1.shp~
- ~model.rf_samplesValid_1.shp~

Those files contain points corresponding to selected samples within
the training and validation polygons. Each point has a set of fields
corresponding to the radiometric measurement at the point location in
the image. Those two files can be displayed in a GIS (in QGIS for
instance).

*** Spot the date with the best performance

The following command line allows to do the training for each date:

#+BEGIN_EXAMPLE
$ for f in $DATA/images/*.tif; do echo $f;            \
      otbcli_TrainImagesClassifier -io.il $f          \
      -io.vd $DATA/references/training/training.shp   \
      -io.valid $DATA/references/testing/testing.shp  \
      -sample.vfn CODE -classifier rf                 \
      -classifier.rf.nbtrees 50 -classifier.rf.max 20 \
      -classifier.rf.cat 13 -io.out model.rf   | grep Kappa; done
#+END_EXAMPLE

Kappa coefficients for each date can be retrieved from the output:

|------------+----------|
|       Date |    Kappa |
|------------+----------|
| 2016-06-07 | 0.609741 |
| 2016-07-07 | 0.615163 |
| 2016-08-06 | 0.593739 |
| 2016-09-05 | 0.614463 |
| 2016-10-05 | 0.622246 |
|------------+----------|

We can see that the coefficient does not vary much, but that the
~2016-10-05~ date has slightly better performances.

*** Classifying and producing a colored classification map

To perform the classification, we use the ~model.rf~ file learnt on
date ~2016-10-05~, and use the following command line:

#+BEGIN_EXAMPLE
$ otbcli_ImageClassifier -in $DATA/images/20161005_T31TCJ_ROI_20m.tif \
                         -out classif_20161005.tif uint8              \
                         -model model.rf
#+END_EXAMPLE

The ~classif_20161005.tif~ contains for each pixel the label of the
class which has been assigned to it. In ordered to ease the
visualisation of the classification result, we can transform this
image by setting a different color for each class, using the *ColorMapping* application:

#+BEGIN_EXAMPLE
$ otbcli_ColorMapping -in classif_20161005.tif            \
                      -out classif_20161005_rgb.tif uint8 \
                      -method custom -method.custom.lut   \ 
                      $DATA/support/color_map.txt
#+END_EXAMPLE

Another way of displaying the ~classif_20161005.tif~ results is to
open it in QGIS and use the style file provided in ~support/classif.qml~.

*** Evaluate global performance

To evaluate global performance over the whole validation set, one can
use the *ComputeConfusionMatrix* application. This application allows
to evaluate any classification map (for instance one that have been
post-processed). Beware not to use as input the colored map created
during previous step, which is only to be used for visualization
purposes.
 
 #+BEGIN_EXAMPLE
 $ otbcli_ComputeConfusionMatrix -in classif_20161005.tif -ref vector  \
                   -ref.vector.in $DATA/references/testing/testing.shp \
                                 -out confusion_20161005.csv           \
                                 -ref.vector.field CODE
#+END_EXAMPLE


The performance measured using the whole validation set is as follows:

#+LATEX: \begin{scriptsize}
#+BEGIN_EXAMPLE
Confusion matrix (rows = reference labels, columns = produced labels):
       [  10] [  31] [  32] [  34] [  36] [  41] [  42] [  43] [  44] [  51] [ 211] [ 221] [ 222]
[  10] 113219    219   2240  10349   4090   2654   2469   1029    233    202   7387   2571   3453
[  31]      8  12282    265     66    346      0      5      1      0      0    174    192     27
[  32]      1     21   1143      5     27      0      2      0      0      0      3      7     10
[  34]    158     47     73   1469    146      9     68     25     12      0    187     11     70
[  36]     11      8      2      8    889      0      4      0      0      0     25      1     11
[  41]     45      0      4     34      7   4637    674    287    135      1      9      9     66
[  42]    800     14    107    675    355   5084  33947   3642   2684     13    468    490   3358
[  43]    816      4     97    417    130   2222   5399  18726   9404     66    137    171    857
[  44]     12      0      7      7      5     54    105    561   2807      6      0     13     27
[  51]    187     26     92     10     73      1     18    249    257  24330      4    300      4
[ 211]    367     73     55    882   1143      9     58      1      0      0   6755    126    301
[ 221]    244    337    372     79    338      2    197     49     16     32    174  10400    398
[ 222]     72      2     66     40    115      9    195      1      0      0     98     52   3474

Precision of class [10] vs all: 0.976531
Recall of class [10] vs all: 0.754215
F-score of class [10] vs all: 0.851095

[...]

Kappa index: 0.659139

#+END_EXAMPLE
#+Latex: \end{scriptsize}

One can note two things:
- First global performance is slightly better than the performance
  assessed during training. This comes from the fact that during the
  training step, we use the same number of samples for all classes,
  whereas when using the *ComputeConfusionMatrix*, all available
  samples are used. Some classes both have more available samples and
  are more easy to classify (such as class 51: water), and are
  therefore improving performances.
- Second, the annual crops class exhibit a lot of confusion with all
  other classes. It has a recall of 0.75 and a precision of 0.97. This
  means that if 97% of pixels identified as annul crops really belong
  to this class, 25% of pixel that belong to this class according to
  the reference validation set have been misclassified. We will see in
  last section that adding multi-temporal information in the
  classification allows to increase this performance.

*** Classification map regularization

To regularize the classification map using a majority voting
algorithm, one can use the following command line:

#+BEGIN_EXAMPLE
$ otbcli_ClassificationMapRegularization -ip.radius 1 -ip.suvbool 0  \
                                         -io.in classif_20161005.tif \
                                         -io.out classif_20161005_reg.tif uint8
#+END_EXAMPLE    

After regularization, we can evaluate the performances of the new
classification map:

#+BEGIN_EXAMPLE
$ otbcli_ComputeConfusionMatrix -in classif_20161005_reg.tif -ref vector  \
                   -ref.vector.in $DATA/references/testing/testing.shp    \
                                 -out confusion_20161005_reg.csv          \
                                 -ref.vector.field CODE

Kappa index: 0.709103
#+END_EXAMPLE

Regularization improves significantly the classification map
performance. This is due to the fact that reference data are rather
regular, and applying this processing makes the classification map
look more like the reference data.

*** Classification with multiple dates


Lets replay the different step with all the dates altogether:

First, training:
#+BEGIN_EXAMPLE
$ otbcli_TrainImagesClassifier -io.il $DATA/images/all.vrt                     \
                               -io.vd $DATA/references/training/training.shp   \
                               -io.valid $DATA/references/testing/testing.shp  \
                               -io.out model_all.rf                            \
                               -sample.vfn CODE -classifier rf                 \
                               -classifier.rf.nbtrees 50 -classifier.rf.max 20 \
                               -classifier.rf.cat 13
#+END_EXAMPLE

Then, classification:
#+BEGIN_EXAMPLE
$ otbcli_ImageClassifier -in $DATA/images/all.vrt    \
                         -out classif_all.tif uint8  \
                         -model model_all.rf
#+END_EXAMPLE

Next, regularisation:
#+BEGIN_EXAMPLE
$ otbcli_ClassificationMapRegularization -ip.radius 1 -ip.suvbool 0  \
                                         -io.in classif_all.tif \
                                         -io.out classif_all_reg.tif uint8
#+END_EXAMPLE   

Finally, evaluation of performances:
#+BEGIN_EXAMPLE
$ otbcli_ComputeConfusionMatrix -in classif_all_reg.tif -ref vector       \
                   -ref.vector.in $DATA/references/testing/testing.shp    \
                                 -out confusion_all_reg.csv               \
                                 -ref.vector.field CODE
#+END_EXAMPLE

#+Latex:\begin{scriptsize}
#+BEGIN_EXAMPLE
Confusion matrix (rows = reference labels, columns = produced labels):
       [  10] [  31] [  32] [  34] [  36] [  41] [  42] [  43] [  44] [  51] [ 211] [ 221] [ 222] 
[  10] 140681     13     68   1893    790    280    494    545     27    108   3569    383   1264 
[  31]     19  12732     87     77    200      3      8      0      0      0    104    131      5 
[  32]      1      6   1211      0      1      0      0      0      0      0      0      0      0 
[  34]      0     32     23   2131     34      0     12     13      0      0     19      5      6 
[  36]      0      0      0      4    937      0      4      0      0      0      8      0      6 
[  41]      2      0      1      0      0   5369    330    111     87      1      0      7      0 
[  42]    148     10     75    465     84   3166  41818   2943   1850      2    316    362    398 
[  43]    143      6     67    528     57   1545   5556  21781   8265     35    166    195    102 
[  44]     11      0     14     13      0     34     60    354   3108      0      2      8      0 
[  51]      7     18     53      0     12      0      3     37     24  25319      2     76      0 
[ 211]    213     41     17    444    491      8     45      1      0      0   8326    104     80 
[ 221]    187     87     66    123    159      0    109     83      3     14    208  11374    225 
[ 222]     29      0      2     15     41      4     42      0      0      0     43     11   3937 

Precision of class [10] vs all: 0.994627
Recall of class [10] vs all: 0.937155
F-score of class [10] vs all: 0.965036

[...]

Kappa index: 0.828411

#+END_EXAMPLE
#+LATEX:\end{scriptsize}

Adding multi-temporal information in classification result in a
significant performance improvement. One can note that recall of the
annual crops class has raised to 93%, which means that now 93% of this
class in the ground truth are correctly classified. This improvement
was expected since crop classes usually have a strong temporal
signature with respect to other classes.

Finally, we can generate the color classification map:

#+BEGIN_EXAMPLE
$ otbcli_ColorMapping -in classif_all_reg.tif            \
                      -out classif_all_reg_rgb.tif uint8 \
                      -method custom -method.custom.lut  \ 
                      $DATA/support/color_map.txt
#+END_EXAMPLE
